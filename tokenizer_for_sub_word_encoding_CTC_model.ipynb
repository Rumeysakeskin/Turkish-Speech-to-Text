{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOcWb+hzfouDzGPpZDMFASk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rumeysakeskin/Turkish-Speech-to-Text/blob/main/tokenizer_for_sub_word_encoding_CTC_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If you're using Google Colab and not running locally, run this cell.\n",
        "\n",
        "## Install dependencies\n",
        "!pip install wget\n",
        "!apt-get install sox libsndfile1 ffmpeg\n",
        "!pip install text-unidecode\n",
        "!pip install matplotlib>=3.3.2\n",
        "\n",
        "## Install NeMo\n",
        "BRANCH = 'main'\n",
        "!python -m pip install git+https://github.com/NVIDIA/NeMo.git@$BRANCH#egg=nemo_toolkit[all]\n",
        "!apt-get update && apt-get install -y libsndfile1 ffmpeg\n",
        "!pip install Cython tensorflow==2.11.0 Pygments==2.6.1 pynini==2.1.5 nemo_toolkit[all]"
      ],
      "metadata": {
        "id": "NIZGA71P2BuA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "geRkdNKbVWeS",
        "outputId": "4bdac241-a1d0-4fc9-c422-455ee3a5cb21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-01-10 08:13:21--  https://raw.githubusercontent.com/NVIDIA/NeMo/main/scripts/tokenizers/process_asr_text_tokenizer.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13860 (14K) [text/plain]\n",
            "Saving to: ‘scripts/process_asr_text_tokenizer.py’\n",
            "\n",
            "process_asr_text_to 100%[===================>]  13.54K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-01-10 08:13:22 (115 MB/s) - ‘scripts/process_asr_text_tokenizer.py’ saved [13860/13860]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "if not os.path.exists(\"scripts/process_asr_text_tokenizer.py\"):\n",
        "  !wget -P scripts/ https://raw.githubusercontent.com/NVIDIA/NeMo/$BRANCH/scripts/tokenizers/process_asr_text_tokenizer.py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LANGUAGE = \"tr\"\n",
        "tokenizer_dir = os.path.join('tokenizers', LANGUAGE)"
      ],
      "metadata": {
        "id": "fCo7WzHRsS1-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Manifest Utils\n",
        "from tqdm.auto import tqdm\n",
        "import json\n",
        "\n",
        "def read_manifest(path):\n",
        "    manifest = []\n",
        "    with open(path, 'r') as f:\n",
        "        for line in tqdm(f, desc=\"Reading manifest data\"):\n",
        "            line = line.replace(\"\\n\", \"\")\n",
        "            data = json.loads(line)\n",
        "            manifest.append(data)\n",
        "    return manifest\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "def get_charset(manifest_data):\n",
        "    charset = defaultdict(int)\n",
        "    for row in tqdm(manifest_data, desc=\"Computing character set\"):\n",
        "        text = row['text']\n",
        "        for character in text:\n",
        "            charset[character] += 1\n",
        "    return charset"
      ],
      "metadata": {
        "id": "xFb9Jju0WNxY"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_manifest = \"scripts/train_manifest.jsonl\"\n",
        "train_manifest_data = read_manifest(train_manifest)\n",
        "train_charset = get_charset(train_manifest_data)\n",
        "train_set = set(train_charset.keys())"
      ],
      "metadata": {
        "id": "4lnvHF5G2ZtX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# << VOCAB SIZE can be changed to any value larger than (len(train_dev_set) + 2)! >>\n",
        "VOCAB_SIZE = len(train_set) + 2\n",
        "VOCAB_SIZE"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTdUlT3uXGup",
        "outputId": "2a5dac52-ba50-4f7e-a271-4f91c56f0066"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "36"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#@title Tokenizer Config { display-mode: \"form\" }\n",
        "TOKENIZER_TYPE = \"bpe\" #@param [\"bpe\", \"unigram\"]"
      ],
      "metadata": {
        "id": "RhNs_ITqXoqe"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python scripts/process_asr_text_tokenizer.py \\\n",
        "  --manifest=$train_manifest \\\n",
        "  --vocab_size=$VOCAB_SIZE \\\n",
        "  --data_root=$tokenizer_dir \\\n",
        "  --tokenizer=\"spe\" \\\n",
        "  --spe_type=$TOKENIZER_TYPE \\\n",
        "  --spe_character_coverage=1.0 \\\n",
        "  --no_lower_case \\\n",
        "  --log"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6s6Q0ihXsQI",
        "outputId": "6927dae4-d61f-4fd7-baa5-8454ed59264c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo W 2023-01-10 08:17:30 optimizers:55] Apex was not found. Using the lamb or fused_adam optimizer will error out.\n",
            "INFO:root:Finished extracting manifest : scripts/semper+common_voice_dataset_100522_NEW.jsonl\n",
            "INFO:root:Finished extracting all manifests ! Number of sentences : 32531\n",
            "[NeMo I 2023-01-10 08:17:30 sentencepiece_tokenizer:315] Processing tokenizers/tr/text_corpus/document.txt and store at tokenizers/tr/tokenizer_spe_bpe_v36\n",
            "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=tokenizers/tr/text_corpus/document.txt --model_prefix=tokenizers/tr/tokenizer_spe_bpe_v36/tokenizer --vocab_size=36 --shuffle_input_sentence=true --hard_vocab_limit=false --model_type=bpe --character_coverage=1.0 --bos_id=-1 --eos_id=-1\n",
            "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
            "trainer_spec {\n",
            "  input: tokenizers/tr/text_corpus/document.txt\n",
            "  input_format: \n",
            "  model_prefix: tokenizers/tr/tokenizer_spe_bpe_v36/tokenizer\n",
            "  model_type: BPE\n",
            "  vocab_size: 36\n",
            "  self_test_sample_size: 0\n",
            "  character_coverage: 1\n",
            "  input_sentence_size: 0\n",
            "  shuffle_input_sentence: 1\n",
            "  seed_sentencepiece_size: 1000000\n",
            "  shrinking_factor: 0.75\n",
            "  max_sentence_length: 4192\n",
            "  num_threads: 16\n",
            "  num_sub_iterations: 2\n",
            "  max_sentencepiece_length: 16\n",
            "  split_by_unicode_script: 1\n",
            "  split_by_number: 1\n",
            "  split_by_whitespace: 1\n",
            "  split_digits: 0\n",
            "  treat_whitespace_as_suffix: 0\n",
            "  allow_whitespace_only_pieces: 0\n",
            "  required_chars: \n",
            "  byte_fallback: 0\n",
            "  vocabulary_output_piece_score: 1\n",
            "  train_extremely_large_corpus: 0\n",
            "  hard_vocab_limit: 0\n",
            "  use_all_vocab: 0\n",
            "  unk_id: 0\n",
            "  bos_id: -1\n",
            "  eos_id: -1\n",
            "  pad_id: -1\n",
            "  unk_piece: <unk>\n",
            "  bos_piece: <s>\n",
            "  eos_piece: </s>\n",
            "  pad_piece: <pad>\n",
            "  unk_surface:  ⁇ \n",
            "  enable_differential_privacy: 0\n",
            "  differential_privacy_noise_level: 0\n",
            "  differential_privacy_clipping_threshold: 0\n",
            "}\n",
            "normalizer_spec {\n",
            "  name: nmt_nfkc\n",
            "  add_dummy_prefix: 1\n",
            "  remove_extra_whitespaces: 1\n",
            "  escape_whitespaces: 1\n",
            "  normalization_rule_tsv: \n",
            "}\n",
            "denormalizer_spec {}\n",
            "trainer_interface.cc(350) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
            "trainer_interface.cc(181) LOG(INFO) Loading corpus: tokenizers/tr/text_corpus/document.txt\n",
            "trainer_interface.cc(406) LOG(INFO) Loaded all 32531 sentences\n",
            "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <unk>\n",
            "trainer_interface.cc(427) LOG(INFO) Normalizing sentences...\n",
            "trainer_interface.cc(536) LOG(INFO) all chars count=808011\n",
            "trainer_interface.cc(557) LOG(INFO) Alphabet size=34\n",
            "trainer_interface.cc(558) LOG(INFO) Final character coverage=1\n",
            "trainer_interface.cc(590) LOG(INFO) Done! preprocessed 32531 sentences.\n",
            "trainer_interface.cc(596) LOG(INFO) Tokenizing input sentences with whitespace: 32531\n",
            "trainer_interface.cc(607) LOG(INFO) Done! 19752\n",
            "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=16082 min_freq=1\n",
            "trainer_interface.cc(685) LOG(INFO) Saving model: tokenizers/tr/tokenizer_spe_bpe_v36/tokenizer.model\n",
            "trainer_interface.cc(697) LOG(INFO) Saving vocabs: tokenizers/tr/tokenizer_spe_bpe_v36/tokenizer.vocab\n",
            "Serialized tokenizer at location : tokenizers/tr/tokenizer_spe_bpe_v36\n",
            "INFO:root:Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TOKENIZER_DIR = f\"{tokenizer_dir}/tokenizer_spe_{TOKENIZER_TYPE}_v{VOCAB_SIZE}/\"\n",
        "print(\"Tokenizer directory :\", TOKENIZER_DIR)"
      ],
      "metadata": {
        "id": "1M8hJq47X6Wl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cea4b49-e5f9-4528-e7b5-defdcb68ff11"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer directory : tokenizers/tr/tokenizer_spe_bpe_v36/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of tokens in tokenizer - \n",
        "with open(os.path.join(TOKENIZER_DIR, 'tokenizer.vocab')) as f:\n",
        "  tokens = f.readlines()\n",
        "\n",
        "num_tokens = len(tokens)\n",
        "print(\"Number of tokens : \", num_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlWFVDu10l8d",
        "outputId": "65bab1d5-69ea-4b4e-c6a0-62c6c547ea09"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of tokens :  36\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if num_tokens < VOCAB_SIZE:\n",
        "    print(\n",
        "        f\"The text in this dataset is too small to construct a tokenizer \"\n",
        "        f\"with vocab size = {VOCAB_SIZE}. Current number of tokens = {num_tokens}. \"\n",
        "        f\"Please reconstruct the tokenizer with fewer tokens\"\n",
        "    )"
      ],
      "metadata": {
        "id": "nF-QtAJ90nx5"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}